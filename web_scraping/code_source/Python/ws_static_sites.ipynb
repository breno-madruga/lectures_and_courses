{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping with Python\n",
    "## Speaker: Breno Santana Santos\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting data with Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps needed to get and extract data from websites are:\n",
    "1. Determine your extraction's goal;\n",
    "2. Check whether your extraction is legal;\n",
    "3. Determine the data source or the target website;\n",
    "4. Get the HTML source of target page;\n",
    "5. Choose the HTML elements that will be extracted;\n",
    "6. Extract the data with your preffered tool of Web Scraping;\n",
    "7. Store the data, if it is necessary.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Required Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required background to perform the activities of Web Scraping are:\n",
    "* Understand the structure (hierarchy) and elements of HTML (HyperText Markup Language);\n",
    "* Understand the syntax of XPath and/or CSS Selector;\n",
    "    * For select/extract the data contained in HTML elements.\n",
    "* Know how to use the needed tools (in this case, Python tools for Web Scraping).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HTML hierarchy and elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tags_html.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of HTML hierarchy:\n",
    "<img src=\"img/structure_html.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, there are two attributes mostly important: class and id.\n",
    " * class: determine the CSS class of HTML element.\n",
    " * id: unique identificator of HTML element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have the deep understanding of HTML, I suggest you take the [W3Schools' HTML tutorial](https://www.w3schools.com/html/).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Review of XPath and CSS Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both resources are used to navigate through elements and attributes in HTML pages. In particular, the CSS Locator enables to select the elements based their CSS styles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic syntax to select the nodes/elements in HTML document is:\n",
    "\n",
    "| Expression                                     | Description                                                                                                             |\n",
    "|------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|\n",
    "| nodename                                       | selects all nodes with the name \"nodename\".                                                                             |\n",
    "| /                                              | selects from the root node.                                                                                             |\n",
    "| //                                             | selects all nodes no matter where they are in the document.                                                             |\n",
    "| .                                              | selects the current node.                                                                                               |\n",
    "| ..                                             | selects the parent of the current node.                                                                                 |\n",
    "| @                                              | selects attributes.                                                                                                     |\n",
    "| parent/nodename[n]                             | selects the n-th \"nodename\" element that is the child of the \"parent\" element. The value of \"n\" starts from one.        |\n",
    "| //nodename[@attr_name]                         | selects all the \"nodename\" elements that have an attribute named \"attr_name\".                                           |\n",
    "| //nodename[@attr_name='attr_value']            | selects all the \"nodename\" elements that have a \"attr_name\" attribute with a value of \"attr_value\".                     |\n",
    "| //nodename[contains(@attr_name, 'attr_value')] | selects all the \"nodename\" elements whose the \"attr_name\" attribute contains the substring \"attr_value\". |\n",
    "| *                                              | matches any element node. |\n",
    "| @*                                             | matches any attribute node.|\n",
    "| nodename/@attribute                            | selects the value of the \"attr_name\" attribute for \"nodename\" element. |\n",
    "| nodename/text()                                | extracts the textual information of the \"nodename\" element. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about XPath, I suggest you take [W3Schools' XPath tutorial](https://www.w3schools.com/xml/xpath_intro.asp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. CSS Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic syntax to select the nodes/elements in HTML document is:\n",
    "\n",
    "| Selector | Description |\n",
    "|----------|-------------|\n",
    "| .class_node | selects all elements with class=\"class_node\". |\n",
    "|.class1.class2 | selects all elements with both \"class1\" and \"class2\" set within its class attribute. |\n",
    "| #id_node | selects the element with id=\"id_node\". |\n",
    "| * | selects all elements. |\n",
    "| tagname | selects all \"tagname\" elements. |\n",
    "| tagname1,tagname2 | selects all \"tagname1\" elements and all \"tagname2\" elements. |\n",
    "| tagname1 tagname2 | selects all \"tagname2\" elements inside \"tagname1\" elements. |\n",
    "| tagname1 > tagname2 | selects all \"tagname2\" elements where the parent is a \"tagname1\" element. |\n",
    "| [attr_name] | selects all elements with a \"attr_name\" attribute. |\n",
    "| [attr_name=attr_value] | selects all elements with attr_name=\"attr_value\". |\n",
    "| [attr_name~=attr_value] | selects all elements with a \"attr_name\" attribute containing the \"attr_value\" value. |\n",
    "| [attr_name*=attr_value] | selects every element whose \"attr_name\" attribute value contains the substring \"attr_value\". |\n",
    "| nodename:nth-of-type(n) | selects every \"nodename\" element that is the n-th element of its parent. The value of \"n\" starts from one. |\n",
    "| nodename::attr(attr_name) | selects the value of the \"attr_name\" attribute for \"nodename\" element. |\n",
    "| nodename::text | extracts the textual information of the \"nodename\" element. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about CSS Selector, see [W3Schools' CSS Selector Reference](https://www.w3schools.com/cssref/css_selectors.asp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| XPath | CSS Selector | Explanation |\n",
    "|-------|--------------|-------------|\n",
    "| /html/body/div | html > body > div | selects all \"div\" elements that are children of \"body\" tag. |\n",
    "| //table | table | selects all \"table\" elements of HTML page. |\n",
    "| /html/body/div[2]//table | html > body > div:nth-of-type(2) table | selects all \"table\" elements contained in the second \"div\" element of \"body\" tag. |\n",
    "| /html/body/* | html > body > * | selects all children of \"body\" tag. |\n",
    "| //p[@class=\"class-1\"] | p.class-1 | selects all \"p\" elements whose \"class\" attribute is equal to the \"class-1\" class. |\n",
    "| //*[@id=\"uid\"] | *#uid | selects all elements, including their children, whose \"id\" attribute is equal to the \"uid\" value. |\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing/Inspecting the HTML pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important resource for support the activities of Web Scraping is the inspection of HTML pages' elements. This resource allows you to visualize the elements and their attributes, thus, facilitating the process of extraction data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/inspect.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Getting the HTML pages' source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of getting the source of HTML pages is by the use of Python __requests__ module. The __get__ method performs an HTTP request and the __content__ attribute represents the HTML page's content. In addition, you should decode the content for the UTF-8 encoding.\n",
    "\n",
    "Other form of getting the source-code is by the Python __urllib__ library's __request__ module. The __urlopen__ returns the HTTPResponse object, which represents a response of an HTTP request. For obtaining the received content by the request, you should use the __read__ method that represents the set of bytes. Similar to previous way, you should decode the content for the UTF-8 encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:50:34.989555Z",
     "end_time": "2023-11-12T21:50:35.054039Z"
    }
   },
   "outputs": [],
   "source": [
    "# Determining the URL of target page.\n",
    "url = \"https://www.imdb.com/chart/top/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:50:34.993461Z",
     "end_time": "2023-11-12T21:50:35.140431Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the requests module and the request module of urllib library.\n",
    "import requests, urllib.request\n",
    "from urllib.request import Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:50:35.052765Z",
     "end_time": "2023-11-12T21:50:35.146587Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the function that gets source of HTML page.\n",
    "def get_page(url, is_urlopen=False):\n",
    "    \"\"\" This function gets the HTML page's source in the UTF-8 encoding. \"\"\"\n",
    "    # Getting the HTML source.\n",
    "    header = {\"User-Agent\": \"Root Scraping\"}\n",
    "    req = Request(url=url, headers=header)\n",
    "    html = urllib.request.urlopen(req).read() if is_urlopen else requests.get(url, headers=header).content\n",
    "    # Setting the encoding of the HTML page.\n",
    "    html = html.decode(\"utf-8\")\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:50:35.054916Z",
     "end_time": "2023-11-12T21:50:36.763388Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting the page using the requests module.\n",
    "html = get_page(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:50:36.769520Z",
     "end_time": "2023-11-12T21:50:36.775475Z"
    }
   },
   "outputs": [],
   "source": [
    "# Printing the 200 first characters.\n",
    "html[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:50:36.773860Z",
     "end_time": "2023-11-12T21:50:40.543569Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting the page using the urlopen method.\n",
    "html = get_page(url, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:50:40.543646Z",
     "end_time": "2023-11-12T21:50:40.545920Z"
    }
   },
   "outputs": [],
   "source": [
    "# Printing the 200 first characters.\n",
    "html[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extracting data with scrapy Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __Selector__ object is a __scrapy__ library's object that __selects and extracts data__ using XPath or CSS Selector notation. In its constructor method, it is necessary to pass the HTML source to the __text__ attribute. The Selector object has two methods to select HTML elements: __xpath__ and __css__. Both methods return __Selector__ or __SelectorList__ objects. We can use these objects to create new Selector ones of specic pieces of the HTML code. The __extract__ method allows to extract a list of data. While the __extract_first__ extracts the first item of returned list by Selector object.\n",
    "\n",
    "In order to get the absolute URLs from the relative ones, you can use the __urllib__ library's __parse__ module. The __urljoin__ method performs this task, i.e., it gets the absolute URLs by the parent and relative URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:50:40.546370Z",
     "end_time": "2023-11-12T21:50:40.548207Z"
    }
   },
   "outputs": [],
   "source": [
    "# For installing the scrapy library, uncomment the below line.\n",
    "# %pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:50:40.549204Z",
     "end_time": "2023-11-12T21:50:40.802358Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the scrapy Selector and the urljoin method.\n",
    "from scrapy import Selector\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:50:40.803261Z",
     "end_time": "2023-11-12T21:50:40.804610Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the function that gets the links contained in a page.\n",
    "def get_links(url, locator_links, is_css=True, locator_pagination=None):\n",
    "    \"\"\" This method gets the page's links based on a XPath/CSS locator. \"\"\"\n",
    "\n",
    "    # Getting the absolute URLs contained in an HTML page, using XPath or CSS Selector.\n",
    "    html = get_page(url)\n",
    "    sel = Selector(text=html)\n",
    "\n",
    "    # Extracting the relative links.\n",
    "    links = sel.css(locator_links).extract() if is_css else sel.xpath(locator_links).extract()\n",
    "\n",
    "    # Generating the absolute URLs.\n",
    "    links = [urljoin(url, item) for item in links]\n",
    "\n",
    "    # Getting the pagination of absolute URLs.\n",
    "    if locator_pagination:\n",
    "        for link in list(links):\n",
    "            links_pagination = get_links(link, locator_pagination)\n",
    "            if links_pagination and not set(links_pagination).issubset(set(links)):\n",
    "                links.remove(link)\n",
    "                links += links_pagination\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Getting the absolute URLs of the IMDb Top 250 Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:50:40.805776Z",
     "end_time": "2023-11-12T21:50:43.993102Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting the absolute URLs of the IMDb Top 250 Movies.\n",
    "xpath = \"//div[@data-testid='chart-layout-main-column']/ul/li/div[2]/div/div/div[contains(@class, 'ipc-title')]\"\n",
    "xpath += \"/a[@class='ipc-title-link-wrapper']/@href\"\n",
    "links = get_links(url, xpath, False)\n",
    "print(len(links))\n",
    "\n",
    "css = \"div[data-testid='chart-layout-main-column'] > ul > li > div:nth-child(2) > div > div > \"\n",
    "css += \"div[class*='ipc-title'] > a[class='ipc-title-link-wrapper']::attr(href)\"\n",
    "links = get_links(url, css)\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Extracting data of the IMDb Top 250 Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:50:43.993768Z",
     "end_time": "2023-11-12T21:50:43.995445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the function that extracts the data based on a list of CSS Selector.\n",
    "def get_data(url, css_list):\n",
    "    \"\"\" This function gets the content and returns it in the list of data. Also, it performs the text cleaning. \"\"\"\n",
    "    html = get_page(url)\n",
    "    sel = Selector(text=html)\n",
    "    return [tuple(sel.css(item_css).getall()) if sel.css(item_css).getall() else \"\"\n",
    "            for item_css in css_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:50:43.997127Z",
     "end_time": "2023-11-12T21:50:43.998658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the list of CSS Selectors to extract data.\n",
    "css = [\n",
    "    # Title\n",
    "    \"section.ipc-page-section > div:nth-child(2) > div:nth-child(1) > h1[data-testid='hero__pageTitle'] > span::text\",\n",
    "    # Year\n",
    "    \"section.ipc-page-section > div:nth-child(2) > div:nth-child(1) > ul[class*='ipc-inline-list'] > li:nth-child(1) > a::text\",\n",
    "    # Duration\n",
    "    \"section.ipc-page-section > div:nth-child(2) > div:nth-child(1) > ul[class*='ipc-inline-list'] > li:nth-child(3)::text\",\n",
    "    # Grade\n",
    "    \"section.ipc-page-section > div:nth-child(2) > div:nth-child(2) > div > div:nth-child(1) > a > span > div > div:nth-child(2) > div:nth-child(1) > ::text\",\n",
    "    # Popularity\n",
    "    \"section.ipc-page-section > div:nth-child(2) > div:nth-child(2) > div > div:nth-child(3) > a > span > div > div:nth-child(2) > div:nth-child(1)::text\",\n",
    "    # Category\n",
    "    \"section.ipc-page-section > div:nth-child(3) > div:nth-child(2) > div:nth-child(1) > section > div:nth-child(1) > div:nth-child(2) > a > span::text\",\n",
    "    # Description\n",
    "    \"section.ipc-page-section > div:nth-child(3) > div:nth-child(2) > div:nth-child(1) > section > p > span:nth-child(1)::text\",\n",
    "    # Direction\n",
    "    \"section.ipc-page-section > div:nth-child(3) > div:nth-child(2) > div:nth-child(1) > section > div:nth-child(3) > div > ul > li:nth-child(1) > div > ul > li > a::text\",\n",
    "    # Screenwriters\n",
    "    \"section.ipc-page-section > div:nth-child(3) > div:nth-child(2) > div:nth-child(1) > section > div:nth-child(3) > div > ul > li:nth-child(2) > div > ul > li > a::text\",\n",
    "    # Main Actors\n",
    "    \"section.ipc-page-section > div:nth-child(3) > div:nth-child(2) > div:nth-child(1) > section > div:nth-child(3) > div > ul > li:nth-child(3) > div > ul > li > a::text\",\n",
    "    # Main Actors Cast\n",
    "    \"section[data-testid='title-cast'] > div:nth-child(2) > div:nth-child(2) > div[data-testid='title-cast-item'] > div:nth-child(2) > a::text\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:50:44.001458Z",
     "end_time": "2023-11-12T21:58:55.573879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting and extracting the data.\n",
    "data = [[\"Title\",\"Year\",\"Duration\",\"Grade\",\"Popularity\",\"Categories\",\"Description\",\"Directors\",\"Screenwriters\",\"Main_Actors\",\"Main_Cast\"]]\n",
    "data.extend([get_data(link, css) for link in links])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalizing the data.\n",
    "for idx, movie in enumerate(data[1:], start=1):\n",
    "    movie = [None if len(movie[i]) == 0 else movie[i][0] if i not in [3, 5, 7, 8, 9, 10] else movie[i]\n",
    "             for i in range(len(movie))]\n",
    "    movie[3] = \"\".join(movie[3])\n",
    "    data[idx] = movie"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T21:58:55.577044Z",
     "end_time": "2023-11-12T21:58:55.579988Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:58:55.578907Z",
     "end_time": "2023-11-12T21:58:55.587743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Printing the five first records and the number of extracted movies.\n",
    "print(data[:5])\n",
    "print(\"Number of extracted movies:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Storing the data in a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to store the data in a CSV file, you can use the __csv__ module. In every file manipulation, it is necessary to open the file to read or write some content. For this operation, you can invoke the __open__ built-in method to open the file for manipulations. This method returns the __TextIOWrapper__ object that is used to __manipulate__ the __selected file__.\n",
    "\n",
    "The __writer__ method of __csv__ module creates a __writer__ object that stores a content within the file. The __writerow__ method inserts a content line within the selected file. While the __writerows__ method inserts a set of content (rows) within the selected file. Both writing methods receive a list as parameter.\n",
    "\n",
    "Other approach is using the **pandas** library. It is one of main libraries used to Data Science with Python. It allows you to work with the data in the **tabular format**. For this, there is the **DataFrame** object. The **to_csv** function allows saving the data of a **DataFrame** object into a **CSV file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:58:55.585325Z",
     "end_time": "2023-11-12T21:58:56.245131Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the required libraries.\n",
    "import pandas as pd, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:58:56.247911Z",
     "end_time": "2023-11-12T21:58:56.257827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the data collected.\n",
    "pd.DataFrame(data).to_csv(\"data_top_250_movies_imdb.csv\", sep=\"|\", header=0, index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Crawling with scrapy Response objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Response** objects have the **same** functionalities of **Selector** ones. They can use the **xpath**, **css**, **extract** and **extract_first** methods. In addition, the Response object **keeps track of the URL** where the HTML code was loaded from, i.e., it keeps track of the URL within the response **url** variable/attribute. It **helps us move from one site to another**, so that we can \"crawl\" the web while scraping. From the **follow** method, the Response lets us \"follow\" to a new link.\n",
    "\n",
    "The **Spider** class determines how to perform the **crawl** (follow links) and how to **extract data** from the pages (scraping items). For creating your Spider, you should create a subclass of **scrapy.Spider**. For the Spider class, it is necessary to create the **start_requests** and **parse** methods. The **former** defines the **start point to run** the Spider object. While the **latter** are responsible for **handling the HTML** code. It is needed to create **at least one parse function**. The **scrapy.Request** statement creates a response variable for us. The **url** argument tells us which site to scrape. The **callback** argument tells us where to send the response variable for processing. The **cb_kwargs** argument tells us what parameters are required for the **callback** argument and it receives a dictionary object that represents the parameters and their values for **callback** function. The **follow** method accepts the same arguments of **scrapy.Request**.\n",
    "\n",
    "Finally, the **CrawlerProcess** objects of **scrapy.crawler** module are responsible for defining and executing the Spider object. The **crawl** method receives the **Spider's subclass** that will be used to extract data. The **start** method starts the execution of the defined Spider's subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:58:56.258575Z",
     "end_time": "2023-11-12T21:58:56.280928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the required libraries.\n",
    "import scrapy, pandas as pd, csv\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:58:56.281174Z",
     "end_time": "2023-11-12T21:58:56.282988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Determining the URL of target page.\n",
    "url = \"https://www.imdb.com/chart/top/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:58:56.286402Z",
     "end_time": "2023-11-12T21:58:56.287892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definition of Spider class\n",
    "class SpiderIMDbMovies(scrapy.Spider):\n",
    "    name = \"imdb_top_250_movies\"\n",
    "    user_agent = name\n",
    "\n",
    "    # Start point to run the spider.\n",
    "    def start_requests(self):\n",
    "        # Getting the relative URLs of the IMDb Top 250 Movies.\n",
    "        css = \"div[data-testid='chart-layout-main-column'] > ul > li > div:nth-child(2) > div > div > \" \\\n",
    "              \"div[class*='ipc-title'] > a[class='ipc-title-link-wrapper']::attr(href)\"\n",
    "        args = dict(css=css)\n",
    "        yield scrapy.Request(url=url, callback=self.parse_links, cb_kwargs=args)\n",
    "\n",
    "    def parse_links(self, response, css):\n",
    "        # Extracting the relative URLs.\n",
    "        links = response.css(css).getall()\n",
    "\n",
    "        # Defining the CSS Selectors.\n",
    "        css = {\n",
    "            \"title\": \"section.ipc-page-section > div:nth-child(2) > div:nth-child(1) > h1[data-testid='hero__pageTitle'] > span::text\",\n",
    "            \"year\": \"section.ipc-page-section > div:nth-child(2) > div:nth-child(1) > ul[class*='ipc-inline-list'] > li:nth-child(1) > a::text\",\n",
    "            \"duration\": \"section.ipc-page-section > div:nth-child(2) > div:nth-child(1) > ul[class*='ipc-inline-list'] > li:nth-child(3)::text\",\n",
    "            \"grade\": \"section.ipc-page-section > div:nth-child(2) > div:nth-child(2) > div > div:nth-child(1) > a > span > div > div:nth-child(2) > div:nth-child(1) > ::text\",\n",
    "            \"popularity\": \"section.ipc-page-section > div:nth-child(2) > div:nth-child(2) > div > div:nth-child(3) > a > span > div > div:nth-child(2) > div:nth-child(1)::text\",\n",
    "            \"category\": \"section.ipc-page-section > div:nth-child(3) > div:nth-child(2) > div:nth-child(1) > section > div:nth-child(1) > div:nth-child(2) > a > span::text\",\n",
    "            \"description\": \"section.ipc-page-section > div:nth-child(3) > div:nth-child(2) > div:nth-child(1) > section > p > span:nth-child(1)::text\",\n",
    "            \"direction\": \"section.ipc-page-section > div:nth-child(3) > div:nth-child(2) > div:nth-child(1) > section > div:nth-child(3) > \"\n",
    "                         \"div > ul > li:nth-child(1) > div > ul > li > a::text\",\n",
    "            \"screenwriters\": \"section.ipc-page-section > div:nth-child(3) > div:nth-child(2) > div:nth-child(1) > section > div:nth-child(3) > \"\n",
    "                             \"div > ul > li:nth-child(2) > div > ul > li > a::text\",\n",
    "            \"main_actors\": \"section.ipc-page-section > div:nth-child(3) > div:nth-child(2) > div:nth-child(1) > section > div:nth-child(3) > \"\n",
    "                           \"div > ul > li:nth-child(3) > div > ul > li > a::text\",\n",
    "            \"main_actors_cast\": \"section[data-testid='title-cast'] > div:nth-child(2) > div:nth-child(2) > div[data-testid='title-cast-item'] > \"\n",
    "                                \"div:nth-child(2) > a::text\"\n",
    "        }\n",
    "\n",
    "        # Getting the data.\n",
    "        args = dict(css=css)\n",
    "        for link in links:\n",
    "            yield response.follow(url=link, callback=self.parse_data, cb_kwargs=args)\n",
    "\n",
    "    def parse_data(self, response, css):\n",
    "        # Extracting the information and saving them into the list \"data\".\n",
    "        row = [tuple(response.css(css[item]).getall()) if response.css(css[item]).getall() else \"\"\n",
    "               for item in css.keys()]\n",
    "\n",
    "        # Normalizing the information and inserting them into the list \"data\".\n",
    "        row = [None if len(row[i]) == 0 else row[i][0] if i not in [3, 5, 7, 8, 9, 10] else row[i]\n",
    "               for i in range(len(row))]\n",
    "        row[3] = \"\".join(row[3])\n",
    "        data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating the list \"data\" and inserting the header within it.\n",
    "data = [[\"Title\",\"Year\",\"Duration\",\"Grade\",\"Popularity\",\"Categories\",\"Description\",\"Directors\",\"Screenwriters\",\"Main_Actors\",\"Main_Cast\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T21:58:56.288880Z",
     "end_time": "2023-11-12T21:58:56.292920Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:58:56.294390Z",
     "end_time": "2023-11-12T21:59:44.880213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Execution Process to run the spider.\n",
    "process = CrawlerProcess()  # initiating a CrawlerProcess.\n",
    "process.crawl(SpiderIMDbMovies)  # telling the Process which Spider to use.\n",
    "process.start()  # starting the Crawling Process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving the data into a CSV file.\n",
    "pd.DataFrame(data).to_csv(\"data_top_250_movies_imdb_2.csv\", sep=\"|\", header=0, index=False, quoting=csv.QUOTE_ALL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T21:59:44.854963Z",
     "end_time": "2023-11-12T21:59:44.880941Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizing the extracted data with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **pandas** library is one of main libraries used to Data Science with Python. It allows you to work with the data in the **tabular format**. For this, there is the **DataFrame** object. The **read_csv** function allows a **DataFrame** object from the **CSV file**. The **info** method permits you to visualize some informations with respect to the data. The **head** method shows the five first records of your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:59:44.861972Z",
     "end_time": "2023-11-12T21:59:44.880997Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating the DataFrame object from the extracted data (CSV file)\n",
    "data_df = pd.read_csv(\"data_top_250_movies_imdb_2.csv\", delimiter=\"|\", header=0, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:59:44.882922Z",
     "end_time": "2023-11-12T21:59:44.885103Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualizing some information with respect to the DataFrame object\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:59:44.885740Z",
     "end_time": "2023-11-12T21:59:44.891616Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualizing the five first records\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Creating a Word Cloud with NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:59:44.891905Z",
     "end_time": "2023-11-12T21:59:44.893004Z"
    }
   },
   "outputs": [],
   "source": [
    "# Installation of NLTK and WordCloud libraries.\n",
    "# %pip install nltk\n",
    "# %pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:59:44.893761Z",
     "end_time": "2023-11-12T21:59:44.895150Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting the list \"Description\".\n",
    "descriptions = data_df[\"Description\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T21:59:44.896112Z",
     "end_time": "2023-11-12T21:59:49.683478Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the required libraries.\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T22:47:24.122582Z",
     "end_time": "2023-11-12T22:47:24.127861Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting the english stopwords.\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.extend([\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\",\n",
    "                   \"eight\", \"nine\", \"ten\", \"ii\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T22:47:25.005007Z",
     "end_time": "2023-11-12T22:47:25.006132Z"
    }
   },
   "outputs": [],
   "source": [
    "# executing some tasks of text cleaning\n",
    "words = [re.sub(r\"[“”`'…\" + string.punctuation + \"]+\", \"\", word.lower().strip())\n",
    "         for desc in descriptions for word in word_tokenize(desc)]\n",
    "words = [word for word in words if word != \"\" and word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-12T22:47:25.290866Z",
     "end_time": "2023-11-12T22:47:26.297748Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting the word cloud.\n",
    "wordcloud = WordCloud(max_font_size=300, max_words=100, width=1500,\n",
    "                      height=900, stopwords=stop_words, background_color=\"white\").generate(\" \".join(words))\n",
    "\n",
    "# Saving the generated image.\n",
    "wordcloud.to_file(\"tag_cloud.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
